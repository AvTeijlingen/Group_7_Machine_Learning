{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "CiDn2Sk-VWqE"
      },
      "outputs": [],
      "source": [
        "# Run this to use from colab environment\n",
        "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and analyse data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples: 246\n",
            "The number of columns: 494\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 246 entries, GIST-001_0 to GIST-246_0\n",
            "Columns: 494 entries, label to PREDICT_original_phasef_phasesym_entropy_WL3_N5\n",
            "dtypes: float64(468), int64(25), object(1)\n",
            "memory usage: 951.3+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "from load_data import load_data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "# Code used to load the original data and split into train and test set\n",
        "# Was only run once at the start to create test and train dataset\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n",
        "print(data.info())\n",
        "\n",
        "# Standardize the data\n",
        "labels = data['label']\n",
        "values = data.drop(columns=['label'])\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_values = scaler.fit_transform(values)\n",
        "\n",
        "values = pd.DataFrame(scaled_values, columns=values.columns, index=values.index)\n",
        "\n",
        "# Convert 'GIST' and 'non-GIST' to numeric values\n",
        "labels = labels.map({'GIST': 1, 'non-GIST': 0})\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature selection and dimension reduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(246, 493)\n",
            "We removed 13 columns without any variance\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\n# Now we can run the logistic regression model for each feature to get its correlation with the label\\np_values = {}\\n\\nfor column in values.columns:\\n    logit_model = sm.Logit(labels, values[column])\\n    result = logit_model.fit()\\n\\n    p_values[column] = result.pvalues[column]\\n\\nprint(p_values)\\n\\n# Now we remove highly correlated features\\n# Make a correlation matrix to analyse the correlation between the features\\ncorrelation_matrix = values.corr().abs()\\n\\n# Heatmap for the entire correlation matrix (for large datasets, consider reducing the size)\\nplt.figure(figsize=(15, 15))  # Adjust size as needed\\nsns.heatmap(correlation_matrix, annot=False, cmap=\\'coolwarm\\', fmt=\".2f\", cbar_kws={\\'label\\': \\'Correlation Coefficient\\'})\\nplt.show()\\n\\nfeatures_to_remove = []\\n\\nfor feature1 in correlation_matrix.columns:\\n    for feature2 in correlation_matrix.columns:\\n        if feature1 != feature2 and feature1 not in features_to_remove and feature2 not in features_to_remove:\\n            if correlation_matrix[feature1][feature2] > 0.95:\\n                # Remove the feature with the higher p-value\\n                if p_values[feature1] < p_values[feature2]:\\n                    features_to_remove.append(feature1)\\n                else:\\n                    features_to_remove.append(feature2)\\n\\nvalues = values.drop(columns=features_to_remove)\\n\\ncorrelation_matrix_2 = values.corr().abs()\\n\\n# Heatmap for the entire correlation matrix (for large datasets, consider reducing the size)\\nplt.figure(figsize=(15, 15))  # Adjust size as needed\\nsns.heatmap(correlation_matrix_2, annot=False, cmap=\\'coolwarm\\', fmt=\".2f\", cbar_kws={\\'label\\': \\'Correlation Coefficient\\'})\\nplt.show()\\n'"
            ]
          },
          "execution_count": 246,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# First we remove NaNs by averaging\n",
        "values = values.fillna(values.mean())\n",
        "\n",
        "# First we remove features with 0 variance\n",
        "print(values.shape)\n",
        "\n",
        "print(f'We removed {values.loc[:, values.var() == 0].shape[1]} columns without any variance')\n",
        "values = values.loc[:, values.var() != 0]\n",
        "\n",
        "\"\"\"\n",
        "# Now we can run the logistic regression model for each feature to get its correlation with the label\n",
        "p_values = {}\n",
        "\n",
        "for column in values.columns:\n",
        "    logit_model = sm.Logit(labels, values[column])\n",
        "    result = logit_model.fit()\n",
        "\n",
        "    p_values[column] = result.pvalues[column]\n",
        "\n",
        "print(p_values)\n",
        "\n",
        "# Now we remove highly correlated features\n",
        "# Make a correlation matrix to analyse the correlation between the features\n",
        "correlation_matrix = values.corr().abs()\n",
        "\n",
        "# Heatmap for the entire correlation matrix (for large datasets, consider reducing the size)\n",
        "plt.figure(figsize=(15, 15))  # Adjust size as needed\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", cbar_kws={'label': 'Correlation Coefficient'})\n",
        "plt.show()\n",
        "\n",
        "features_to_remove = []\n",
        "\n",
        "for feature1 in correlation_matrix.columns:\n",
        "    for feature2 in correlation_matrix.columns:\n",
        "        if feature1 != feature2 and feature1 not in features_to_remove and feature2 not in features_to_remove:\n",
        "            if correlation_matrix[feature1][feature2] > 0.95:\n",
        "                # Remove the feature with the higher p-value\n",
        "                if p_values[feature1] < p_values[feature2]:\n",
        "                    features_to_remove.append(feature1)\n",
        "                else:\n",
        "                    features_to_remove.append(feature2)\n",
        "\n",
        "values = values.drop(columns=features_to_remove)\n",
        "\n",
        "correlation_matrix_2 = values.corr().abs()\n",
        "\n",
        "# Heatmap for the entire correlation matrix (for large datasets, consider reducing the size)\n",
        "plt.figure(figsize=(15, 15))  # Adjust size as needed\n",
        "sns.heatmap(correlation_matrix_2, annot=False, cmap='coolwarm', fmt=\".2f\", cbar_kws={'label': 'Correlation Coefficient'})\n",
        "plt.show()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Elastic net selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "import numpy as np\n",
        "\n",
        "def elastic_net_feature_selection(training_values, training_labels):\n",
        "\n",
        "    elastic_net = LogisticRegressionCV(\n",
        "    penalty=\"elasticnet\",\n",
        "    solver=\"saga\",         \n",
        "    l1_ratios=[0.8],  \n",
        "    cv=5,                 \n",
        "    max_iter=5000,         \n",
        "    random_state=42\n",
        "    )   \n",
        "\n",
        "    elastic_net.fit(training_values, training_labels)\n",
        "\n",
        "    selected_features = np.where(elastic_net.coef_ != 0)[0]\n",
        "\n",
        "    print(f\"{len(selected_features)} features selected out of {len(training_values.columns)}\")\n",
        "\n",
        "    training_values = training_values.iloc[:, selected_features]\n",
        "\n",
        "    return training_values, training_labels\n",
        "\n",
        "## ALSO: code voor sequential feature selection\n",
        "#sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction='forward', n_jobs=-1)\n",
        "#sfs.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "def train_random_forest(training_values, training_labels):\n",
        "    # Initialize the Random Forest Classifier\n",
        "    param_grid = {\n",
        "        'n_estimators': randint(1, 100),\n",
        "        'max_depth': randint(1, 20),\n",
        "        'max_features': ['sqrt', 'log2'],\n",
        "        'min_samples_leaf': randint(1, 10)\n",
        "        }\n",
        "    \n",
        "    random_search = RandomizedSearchCV(RandomForestClassifier(), param_distributions=param_grid, n_iter=20, cv=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "    random_search.fit(training_values, training_labels)\n",
        "    print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "    # Return best model's cross-validation score\n",
        "    best_model = random_search.best_estimator_\n",
        "\n",
        "    feature_importances = best_model.feature_importances_\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': training_values.columns,\n",
        "        'Importance': feature_importances\n",
        "    })\n",
        "\n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=True)\n",
        "\n",
        "    all_scores = []\n",
        "    for i in range(1, 493):\n",
        "        top_features = feature_importance_df.head(20)['Feature'].values\n",
        "\n",
        "        data_selected = training_values[top_features]\n",
        "        labels_selected = training_labels\n",
        "\n",
        "        final_model = best_model.fit(data_selected, labels_selected)\n",
        "        cv_scores = cross_val_score(best_model, data_selected, labels_selected, cv=5)\n",
        "        all_scores.append(cv_scores.mean())\n",
        "\n",
        "    # Plot the cross-validation scores\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, 200), all_scores, marker='o', linestyle='-', color='b')\n",
        "    plt.xlabel('Number of Top Features')\n",
        "    plt.ylabel('Cross-Validation Score')\n",
        "    plt.title('Cross-Validation Score vs Number of Top Features')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return final_model, cv_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN-classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "def KNN(training_values, training_labels):\n",
        "    #training_values, training_labels = elastic_net_feature_selection(training_values, training_labels)\n",
        "\n",
        "    param_grid = {\n",
        "    'n_neighbors': randint(1, 100),\n",
        "    'weights': ['uniform', 'distance']\n",
        "    }\n",
        "\n",
        "    random_search = RandomizedSearchCV(KNeighborsClassifier(), param_distributions=param_grid, n_iter=20, cv=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "    random_search.fit(training_values, training_labels)\n",
        "    print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "    # Return best model's cross-validation score\n",
        "    best_model = random_search.best_estimator_\n",
        "    print(best_model)\n",
        "    cv_scores = cross_val_score(best_model, training_values, training_labels, cv=5)\n",
        "\n",
        "    return cv_scores.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
        "\n",
        "def SVM(training_values, training_labels):\n",
        "    #training_values, training_labels = elastic_net_feature_selection(training_values, training_labels)\n",
        "\n",
        "    param_grid = {\n",
        "        'C': [0.1,0.5,1,5,10,25,50], # Use a wide range of slacks\n",
        "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'], # the used kernels\n",
        "        'gamma': ['scale', 'auto', 0.01, 0.1, 1],  # SLack variable per object \n",
        "        'degree': [2, 3, 4],  # Only used for 'poly' kernel\n",
        "    }\n",
        "\n",
        "    random_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,n_iter=50, cv=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "    random_search.fit(training_values, training_labels)\n",
        "    print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "    # Return best model's cross-validation score\n",
        "    best_model = random_search.best_estimator_\n",
        "    print(best_model)\n",
        "    cv_scores = cross_val_score(best_model, training_values, training_labels, cv=5)\n",
        "\n",
        "    return cv_scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Linear classifier\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
        "\n",
        "def linear_classifier(training_values,training_labels):\n",
        "    # # === LDA model trainen ===\n",
        "    lda = LinearDiscriminantAnalysis()\n",
        "    lda.fit(training_values, training_labels)  # Model trainen met training set\n",
        "    \n",
        "    param_grid = {\n",
        "        'solver': ['lsqr', 'eigen'],  # Solver types for LDA\n",
        "        'shrinkage': [None, 'auto'],  # Shrinkage types for LDA\n",
        "    }\n",
        "\n",
        "    random_search = RandomizedSearchCV(SVC(), param_distributions=param_grid,n_iter=20, cv=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "    random_search.fit(training_values, training_labels)\n",
        "    print(f\"Best hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "    # Return best model's cross-validation score\n",
        "    best_model = random_search.best_estimator_\n",
        "    print(best_model)\n",
        "    cv_scores = cross_val_score(best_model, training_values, training_labels, cv=5)\n",
        "    \n",
        "    return cv_scores.mean()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Try the different classifiers for 5 different folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Fold 0 ===\n",
            "Best hyperparameters: {'n_estimators': 85, 'min_samples_leaf': 9, 'max_features': 'log2', 'max_depth': 9}\n",
            "Fold 0 - Random Forest Score: 0.5913\n",
            "100 features selected out of 480\n",
            "Best hyperparameters: {'n_neighbors': 72, 'weights': 'uniform'}\n",
            "KNeighborsClassifier(n_neighbors=72)\n",
            "Fold 0 - KNN Score: 0.5764\n",
            "100 features selected out of 480\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# 5-fold cross-validation setup\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(skf.split(values, labels)):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "\n",
        "    training_values = values.iloc[train_index]\n",
        "    training_labels = labels[train_index]\n",
        "\n",
        "    test_values = values.iloc[test_index]\n",
        "    test_labels = labels[test_index]\n",
        "\n",
        "    forest_model, forest_score = train_random_forest(training_values, training_labels)\n",
        "    print(f\"Fold {fold} - Random Forest Score: {forest_score:.4f}\")\n",
        "\n",
        "    knn_score = KNN(training_values, training_labels)\n",
        "    print(f\"Fold {fold} - KNN Score: {knn_score:.4f}\")\n",
        "\n",
        "    svm_score = SVM(training_values, training_labels)\n",
        "    print(f\"Fold {fold} - SVM Score: {svm_score:.4f}\")\n",
        "\n",
        "    lda_score = linear_classifier(training_values, training_labels)\n",
        "    print(f\"Fold {fold} - LDA Score: {lda_score:.4f}\")\n",
        "    \n",
        "#)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
