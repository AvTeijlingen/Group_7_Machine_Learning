{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Data inladen\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Data voorbereiden\n",
    "training_label = train_data['label']\n",
    "training_values = train_data.drop(columns=['label'], axis=1)\n",
    "\n",
    "# Gebruik Random Forest om feature importances te verkrijgen\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(training_values, training_label)\n",
    "\n",
    "# Verkrijg de belangrijkheid van de features\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# Selecteer de top K belangrijkste features (bijv. 10 belangrijkste)\n",
    "k = 10  # Pas dit aan afhankelijk van hoeveel features je wilt behouden\n",
    "indices = np.argsort(feature_importances)[::-1][:k]\n",
    "training_values_selected = training_values.iloc[:, indices]\n",
    "\n",
    "# Als er nog steeds meer dan 2 features zijn, reduceren we naar 2D met PCA\n",
    "if training_values_selected.shape[1] > 2:\n",
    "    pca = PCA(n_components=2)\n",
    "    training_values_selected = pca.fit_transform(training_values_selected)\n",
    "\n",
    "# Split de data in training en test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_values_selected, training_label, test_size=0.3, random_state=42)\n",
    "\n",
    "# Maak een Soft Margin SVM model aan\n",
    "svm = SVC(C=1.0, kernel='linear', random_state=42)\n",
    "\n",
    "# Train het model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Voorspel op de test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Bereken de nauwkeurigheid van het model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy van de Soft Margin SVM classifier met feature selectie: {accuracy:.2f}')\n",
    "\n",
    "# Plot de beslissinggrens en de data\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bereken de besluitlijn (marge) voor de classifier\n",
    "x_min, x_max = X_train[:, 0].min() - 1, X_train[:, 0].max() + 1\n",
    "y_min, y_max = X_train[:, 1].min() - 1, X_train[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "# Plot de beslissinggrens\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.coolwarm)\n",
    "\n",
    "# Plot de trainingsdata\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, marker='o', edgecolors='k', cmap=plt.cm.coolwarm, s=50)\n",
    "\n",
    "# Voeg titels en labels toe\n",
    "plt.title('Besluitlijn van de Soft Margin SVM met Feature Selectie via Random Forest', fontsize=14)\n",
    "plt.xlabel('Feature 1', fontsize=12)\n",
    "plt.ylabel('Feature 2', fontsize=12)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
